{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varsigma</th>\n",
       "      <th>kappa</th>\n",
       "      <th>delta</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>tau</th>\n",
       "      <th>stockPrice</th>\n",
       "      <th>strike</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>150</td>\n",
       "      <td>1.20</td>\n",
       "      <td>45.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>160</td>\n",
       "      <td>1.12</td>\n",
       "      <td>39.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>170</td>\n",
       "      <td>1.06</td>\n",
       "      <td>32.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>180</td>\n",
       "      <td>1.00</td>\n",
       "      <td>27.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.16</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>180.00</td>\n",
       "      <td>190</td>\n",
       "      <td>0.95</td>\n",
       "      <td>22.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748995</th>\n",
       "      <td>0.34</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.41</td>\n",
       "      <td>200</td>\n",
       "      <td>0.86</td>\n",
       "      <td>25.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748996</th>\n",
       "      <td>0.34</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.41</td>\n",
       "      <td>210</td>\n",
       "      <td>0.82</td>\n",
       "      <td>22.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748997</th>\n",
       "      <td>0.34</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.41</td>\n",
       "      <td>220</td>\n",
       "      <td>0.78</td>\n",
       "      <td>19.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748998</th>\n",
       "      <td>0.34</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.41</td>\n",
       "      <td>230</td>\n",
       "      <td>0.75</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748999</th>\n",
       "      <td>0.34</td>\n",
       "      <td>2.23</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>171.41</td>\n",
       "      <td>250</td>\n",
       "      <td>0.69</td>\n",
       "      <td>12.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1749000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         varsigma  kappa  delta    v0   rho  tau  stockPrice  strike  \\\n",
       "0            0.16   1.78   0.61  0.06 -0.62  1.0      180.00     150   \n",
       "1            0.16   1.78   0.61  0.06 -0.62  1.0      180.00     160   \n",
       "2            0.16   1.78   0.61  0.06 -0.62  1.0      180.00     170   \n",
       "3            0.16   1.78   0.61  0.06 -0.62  1.0      180.00     180   \n",
       "4            0.16   1.78   0.61  0.06 -0.62  1.0      180.00     190   \n",
       "...           ...    ...    ...   ...   ...  ...         ...     ...   \n",
       "1748995      0.34   2.23   0.55  0.14 -0.83  1.0      171.41     200   \n",
       "1748996      0.34   2.23   0.55  0.14 -0.83  1.0      171.41     210   \n",
       "1748997      0.34   2.23   0.55  0.14 -0.83  1.0      171.41     220   \n",
       "1748998      0.34   2.23   0.55  0.14 -0.83  1.0      171.41     230   \n",
       "1748999      0.34   2.23   0.55  0.14 -0.83  1.0      171.41     250   \n",
       "\n",
       "         moneyness  price  \n",
       "0             1.20  45.78  \n",
       "1             1.12  39.00  \n",
       "2             1.06  32.78  \n",
       "3             1.00  27.17  \n",
       "4             0.95  22.18  \n",
       "...            ...    ...  \n",
       "1748995       0.86  25.49  \n",
       "1748996       0.82  22.21  \n",
       "1748997       0.78  19.28  \n",
       "1748998       0.75  16.67  \n",
       "1748999       0.69  12.33  \n",
       "\n",
       "[1749000 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"csv_files/train.csv\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varsigma</th>\n",
       "      <th>kappa</th>\n",
       "      <th>delta</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>tau</th>\n",
       "      <th>stockPrice</th>\n",
       "      <th>strike</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "      <td>1.749000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.770000e-01</td>\n",
       "      <td>1.710900e+00</td>\n",
       "      <td>3.634333e-01</td>\n",
       "      <td>9.103333e-02</td>\n",
       "      <td>-5.388667e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.795318e+02</td>\n",
       "      <td>1.960000e+02</td>\n",
       "      <td>9.384750e-01</td>\n",
       "      <td>2.772866e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.372941e-01</td>\n",
       "      <td>7.905263e-01</td>\n",
       "      <td>2.244740e-01</td>\n",
       "      <td>3.526943e-02</td>\n",
       "      <td>2.017475e-01</td>\n",
       "      <td>4.316752e-01</td>\n",
       "      <td>1.251597e+01</td>\n",
       "      <td>3.039738e+01</td>\n",
       "      <td>1.608779e-01</td>\n",
       "      <td>1.714504e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>3.000000e-02</td>\n",
       "      <td>-9.000000e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.500100e+02</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>6.000000e-01</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.600000e-01</td>\n",
       "      <td>1.097500e+00</td>\n",
       "      <td>1.700000e-01</td>\n",
       "      <td>6.000000e-02</td>\n",
       "      <td>-7.100000e-01</td>\n",
       "      <td>6.900000e-01</td>\n",
       "      <td>1.704700e+02</td>\n",
       "      <td>1.700000e+02</td>\n",
       "      <td>8.100000e-01</td>\n",
       "      <td>1.399000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.800000e-01</td>\n",
       "      <td>1.765000e+00</td>\n",
       "      <td>3.400000e-01</td>\n",
       "      <td>9.000000e-02</td>\n",
       "      <td>-5.400000e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.796900e+02</td>\n",
       "      <td>1.950000e+02</td>\n",
       "      <td>9.200000e-01</td>\n",
       "      <td>2.619000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e-01</td>\n",
       "      <td>2.392500e+00</td>\n",
       "      <td>5.525000e-01</td>\n",
       "      <td>1.200000e-01</td>\n",
       "      <td>-3.700000e-01</td>\n",
       "      <td>1.310000e+00</td>\n",
       "      <td>1.885400e+02</td>\n",
       "      <td>2.200000e+02</td>\n",
       "      <td>1.060000e+00</td>\n",
       "      <td>3.973000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>7.900000e-01</td>\n",
       "      <td>1.500000e-01</td>\n",
       "      <td>-2.000000e-01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.099700e+02</td>\n",
       "      <td>2.500000e+02</td>\n",
       "      <td>1.400000e+00</td>\n",
       "      <td>1.006500e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           varsigma         kappa         delta            v0           rho  \\\n",
       "count  1.749000e+06  1.749000e+06  1.749000e+06  1.749000e+06  1.749000e+06   \n",
       "mean   2.770000e-01  1.710900e+00  3.634333e-01  9.103333e-02 -5.388667e-01   \n",
       "std    1.372941e-01  7.905263e-01  2.244740e-01  3.526943e-02  2.017475e-01   \n",
       "min    1.000000e-02  3.000000e-02  1.000000e-02  3.000000e-02 -9.000000e-01   \n",
       "25%    1.600000e-01  1.097500e+00  1.700000e-01  6.000000e-02 -7.100000e-01   \n",
       "50%    2.800000e-01  1.765000e+00  3.400000e-01  9.000000e-02 -5.400000e-01   \n",
       "75%    4.000000e-01  2.392500e+00  5.525000e-01  1.200000e-01 -3.700000e-01   \n",
       "max    5.000000e-01  3.000000e+00  7.900000e-01  1.500000e-01 -2.000000e-01   \n",
       "\n",
       "                tau    stockPrice        strike     moneyness         price  \n",
       "count  1.749000e+06  1.749000e+06  1.749000e+06  1.749000e+06  1.749000e+06  \n",
       "mean   1.000000e+00  1.795318e+02  1.960000e+02  9.384750e-01  2.772866e+01  \n",
       "std    4.316752e-01  1.251597e+01  3.039738e+01  1.608779e-01  1.714504e+01  \n",
       "min    0.000000e+00  1.500100e+02  1.500000e+02  6.000000e-01 -0.000000e+00  \n",
       "25%    6.900000e-01  1.704700e+02  1.700000e+02  8.100000e-01  1.399000e+01  \n",
       "50%    1.000000e+00  1.796900e+02  1.950000e+02  9.200000e-01  2.619000e+01  \n",
       "75%    1.310000e+00  1.885400e+02  2.200000e+02  1.060000e+00  3.973000e+01  \n",
       "max    2.000000e+00  2.099700e+02  2.500000e+02  1.400000e+00  1.006500e+02  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain features and label -> X, Y\n",
    "X, Y = train_data.drop([\"moneyness\", \"price\"], axis=1), train_data[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data -> X_normalized, Y_normalized\n",
    "## Normalize the model parameters in a domain\n",
    "def custom_min_max_normalization(x, xmin, xmax):\n",
    "    return (2 * x - (xmax + xmin)) / (xmax - xmin)\n",
    "\n",
    "model_parameters = X.drop([\"tau\", \"stockPrice\", \"strike\"], axis=1)\n",
    "option_properties = X.drop([\"varsigma\", \"kappa\", \"delta\", \"v0\", \"rho\"], axis=1)\n",
    "\n",
    "min_vals = pd.Series({ \n",
    "    'varsigma': 0.01,\n",
    "    'kappa': 0,\n",
    "    'v0': 0.03,\n",
    "    'delta': 0.01,\n",
    "    'rho': -0.9\n",
    "})\n",
    "\n",
    "max_vals = pd.Series({\n",
    "    'varsigma': 0.5,\n",
    "    'kappa': 3.0,\n",
    "    'v0': 0.15,\n",
    "    'delta': 0.8,\n",
    "    'rho': -0.2\n",
    "})\n",
    "\n",
    "normalized_model_parameters = pd.DataFrame()\n",
    "for column in model_parameters.columns:\n",
    "    normalized_model_parameters[column] = custom_min_max_normalization(\n",
    "        model_parameters[column], \n",
    "        min_vals[column], \n",
    "        max_vals[column]\n",
    "    )\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "normalized_option_properties = scaler.fit_transform(option_properties)\n",
    "normalized_option_properties = pd.DataFrame(normalized_option_properties, columns=option_properties.columns)\n",
    "\n",
    "X_normalized = pd.concat([normalized_model_parameters, normalized_option_properties], axis=1).values\n",
    "\n",
    "Y_normalized = scaler.fit_transform(Y.values.reshape(-1, 1))\n",
    "Y_normalized = pd.Series(Y_normalized.flatten(), name=Y.name).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequences of days\n",
    "num_features = X.shape[1]\n",
    "num_days = 53 # number of days\n",
    "num_samples = len(X)\n",
    "num_samples_per_day = num_samples // num_days\n",
    "\n",
    "X_days = [X_normalized[i*num_samples_per_day : (i+1)*num_samples_per_day] for i in range(num_days)]\n",
    "Y_days = [Y_normalized[i*num_samples_per_day : (i+1)*num_samples_per_day] for i in range(num_days)]\n",
    "\n",
    "X_seq = np.stack(X_days, axis=1)\n",
    "Y_seq = np.stack(Y_days, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"csv_files/outOfSample.csv\")\n",
    "test = round(test,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varsigma</th>\n",
       "      <th>kappa</th>\n",
       "      <th>delta</th>\n",
       "      <th>v0</th>\n",
       "      <th>rho</th>\n",
       "      <th>tau</th>\n",
       "      <th>stockPrice</th>\n",
       "      <th>strike</th>\n",
       "      <th>moneyness</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.00000</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.00000</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.000000</td>\n",
       "      <td>42400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.300600</td>\n",
       "      <td>1.72840</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>0.09240</td>\n",
       "      <td>-0.525600</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>176.103230</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.939944</td>\n",
       "      <td>30.159162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.130192</td>\n",
       "      <td>0.76819</td>\n",
       "      <td>0.225335</td>\n",
       "      <td>0.03592</td>\n",
       "      <td>0.194786</td>\n",
       "      <td>0.422904</td>\n",
       "      <td>10.120805</td>\n",
       "      <td>22.360943</td>\n",
       "      <td>0.124657</td>\n",
       "      <td>15.160794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.03000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>155.110000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.210000</td>\n",
       "      <td>1.23000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.06000</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>168.540000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>18.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.305000</td>\n",
       "      <td>1.80000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>-0.520000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>175.160000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>29.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.28000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>1.395000</td>\n",
       "      <td>183.490000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>40.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.490000</td>\n",
       "      <td>2.99000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.15000</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>199.870000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           varsigma        kappa         delta           v0           rho  \\\n",
       "count  42400.000000  42400.00000  42400.000000  42400.00000  42400.000000   \n",
       "mean       0.300600      1.72840      0.346600      0.09240     -0.525600   \n",
       "std        0.130192      0.76819      0.225335      0.03592      0.194786   \n",
       "min        0.040000      0.10000      0.010000      0.03000     -0.870000   \n",
       "25%        0.210000      1.23000      0.150000      0.06000     -0.700000   \n",
       "50%        0.305000      1.80000      0.310000      0.09000     -0.520000   \n",
       "75%        0.420000      2.28000      0.520000      0.12000     -0.350000   \n",
       "max        0.490000      2.99000      0.780000      0.15000     -0.210000   \n",
       "\n",
       "                tau    stockPrice        strike     moneyness         price  \n",
       "count  42400.000000  42400.000000  42400.000000  42400.000000  42400.000000  \n",
       "mean       1.100000    176.103230    190.000000      0.939944     30.159162  \n",
       "std        0.422904     10.120805     22.360943      0.124657     15.160794  \n",
       "min        0.250000    155.110000    160.000000      0.700000      0.010000  \n",
       "25%        0.805000    168.540000    175.000000      0.840000     18.660000  \n",
       "50%        1.100000    175.160000    190.000000      0.930000     29.560000  \n",
       "75%        1.395000    183.490000    205.000000      1.040000     40.860000  \n",
       "max        1.950000    199.870000    220.000000      1.250000     81.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestX, TestY = test.drop([\"moneyness\",\"price\"], axis=1), test[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize TestX\n",
    "test_model_parameters = TestX.drop([\"tau\", \"stockPrice\", \"strike\"], axis=1)\n",
    "test_option_properties = TestX.drop([\"varsigma\", \"kappa\", \"delta\", \"v0\", \"rho\"], axis=1)\n",
    "\n",
    "normalized_test_model_parameters = pd.DataFrame()\n",
    "for column in test_model_parameters.columns:\n",
    "    normalized_test_model_parameters[column] = custom_min_max_normalization(\n",
    "        test_model_parameters[column], \n",
    "        min_vals[column], \n",
    "        max_vals[column]\n",
    "    )\n",
    "\n",
    "# Fit the scaler with option_properties from the training data\n",
    "scaler_option_properties = MinMaxScaler() \n",
    "scaler_option_properties.fit(option_properties)\n",
    "\n",
    "# Transform the option_properties from the test data\n",
    "normalized_test_option_properties = scaler_option_properties.transform(test_option_properties)\n",
    "normalized_test_option_properties = pd.DataFrame(normalized_test_option_properties, columns=test_option_properties.columns)\n",
    "\n",
    "TestX_normalized = pd.concat([normalized_test_model_parameters, normalized_test_option_properties], axis=1).values\n",
    "\n",
    "# Now the scaler for Y\n",
    "scaler_Y = MinMaxScaler()\n",
    "scaler_Y.fit(Y.values.reshape(-1, 1))\n",
    "\n",
    "# Normalize TestY\n",
    "TestY_normalized = scaler_Y.transform(TestY.values.reshape(-1, 1))  # Use transform, not fit_transform\n",
    "TestY_normalized = pd.Series(TestY_normalized.flatten(), name=TestY.name).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sequences of days\n",
    "Testnum_features = TestX.shape[1]\n",
    "Testnum_days = 53 # number of days\n",
    "Testnum_samples = len(TestX)\n",
    "Testnum_samples_per_day = Testnum_samples // Testnum_days\n",
    "\n",
    "TestX_days = [TestX_normalized[i*Testnum_samples_per_day : (i+1)*Testnum_samples_per_day] for i in range(Testnum_days)]\n",
    "TestY_days = [TestY_normalized[i*Testnum_samples_per_day : (i+1)*Testnum_samples_per_day] for i in range(Testnum_days)]\n",
    "\n",
    "TestX_seq = np.stack(TestX_days, axis=1)\n",
    "TestY_seq = np.stack(TestY_days, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 layers, nodes = 128, epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m70,144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,441</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m333,441\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,441</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m333,441\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 170ms/step - loss: 0.0080 - val_loss: 2.3051e-04 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 164ms/step - loss: 1.2127e-04 - val_loss: 1.8980e-04 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 148ms/step - loss: 7.5849e-05 - val_loss: 6.4865e-05 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 142ms/step - loss: 4.3638e-05 - val_loss: 1.0763e-04 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 152ms/step - loss: 4.1253e-05 - val_loss: 4.1502e-05 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 154ms/step - loss: 2.9398e-05 - val_loss: 5.0756e-05 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 172ms/step - loss: 2.3164e-05 - val_loss: 3.3601e-05 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 152ms/step - loss: 2.3211e-05 - val_loss: 4.2661e-05 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 151ms/step - loss: 1.4161e-05 - val_loss: 2.8582e-05 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 154ms/step - loss: 1.6385e-05 - val_loss: 2.4892e-05 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 155ms/step - loss: 1.5757e-05 - val_loss: 2.7949e-05 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 140ms/step - loss: 1.1932e-05 - val_loss: 2.8790e-05 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 146ms/step - loss: 1.3615e-05 - val_loss: 1.6144e-05 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 150ms/step - loss: 9.9654e-06 - val_loss: 1.3563e-05 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 155ms/step - loss: 1.0541e-05 - val_loss: 1.8537e-05 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 161ms/step - loss: 1.1046e-05 - val_loss: 1.6398e-05 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 151ms/step - loss: 6.2378e-06 - val_loss: 1.2895e-05 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 146ms/step - loss: 1.4942e-05 - val_loss: 1.7688e-05 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 149ms/step - loss: 6.8744e-06 - val_loss: 1.0715e-05 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 147ms/step - loss: 8.2044e-06 - val_loss: 1.0578e-05 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 147ms/step - loss: 2.2615e-06 - val_loss: 1.0214e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 156ms/step - loss: 2.0033e-06 - val_loss: 9.0743e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 154ms/step - loss: 2.3628e-06 - val_loss: 8.6100e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 148ms/step - loss: 2.7277e-06 - val_loss: 7.8536e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 142ms/step - loss: 2.0194e-06 - val_loss: 8.0705e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 138ms/step - loss: 3.5226e-06 - val_loss: 1.1420e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 140ms/step - loss: 2.6840e-06 - val_loss: 7.2617e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 122ms/step - loss: 2.6907e-06 - val_loss: 5.4638e-05 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 140ms/step - loss: 3.9181e-06 - val_loss: 6.6600e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 134ms/step - loss: 1.6547e-06 - val_loss: 7.5666e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 133ms/step - loss: 2.0909e-06 - val_loss: 6.3713e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 116ms/step - loss: 1.5278e-06 - val_loss: 6.9022e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 107ms/step - loss: 2.1527e-06 - val_loss: 5.9271e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 105ms/step - loss: 2.4020e-06 - val_loss: 5.8112e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 146ms/step - loss: 1.8061e-06 - val_loss: 6.0020e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 135ms/step - loss: 1.2137e-06 - val_loss: 6.1026e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 118ms/step - loss: 1.9479e-06 - val_loss: 5.4881e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 121ms/step - loss: 2.0223e-06 - val_loss: 5.0203e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 123ms/step - loss: 1.8518e-06 - val_loss: 4.8611e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 141ms/step - loss: 2.0560e-06 - val_loss: 4.9341e-06 - learning_rate: 5.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 135ms/step - loss: 8.0749e-07 - val_loss: 4.8290e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 129ms/step - loss: 7.8570e-07 - val_loss: 4.1925e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 133ms/step - loss: 8.8884e-07 - val_loss: 5.1173e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 125ms/step - loss: 9.7129e-07 - val_loss: 3.9414e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 132ms/step - loss: 9.6459e-07 - val_loss: 3.8992e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 130ms/step - loss: 8.2956e-07 - val_loss: 3.8792e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 124ms/step - loss: 9.1534e-07 - val_loss: 4.7515e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 123ms/step - loss: 8.7923e-07 - val_loss: 4.1724e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 135ms/step - loss: 8.2882e-07 - val_loss: 3.5772e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 138ms/step - loss: 8.0529e-07 - val_loss: 4.1591e-06 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Data preprossesing -> X_train, X_test, X_val, Y_train, Y_test, Y_val\n",
    "# Train - 80% dos dias, Test - 10% dos dias, Validation - 10% dos dias\n",
    "train_size_per_day = int(0.8 * num_samples_per_day)\n",
    "val_size_per_day = int(0.1 * num_samples_per_day)\n",
    "test_size_per_day = num_samples_per_day - train_size_per_day - val_size_per_day\n",
    "\n",
    "X_train = X_seq[:train_size_per_day]\n",
    "Y_train = Y_seq[:train_size_per_day]\n",
    "\n",
    "X_val = X_seq[train_size_per_day : train_size_per_day+val_size_per_day]\n",
    "Y_val = Y_seq[train_size_per_day : train_size_per_day+val_size_per_day]\n",
    "\n",
    "X_test = X_seq[train_size_per_day+val_size_per_day:]\n",
    "Y_test = Y_seq[train_size_per_day+val_size_per_day:]\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "lstm.add(LSTM(units=128, return_sequences=True))\n",
    "lstm.add(LSTM(units=128, return_sequences=True))\n",
    "lstm.add(Dense(1,activation=tf.nn.relu))\n",
    "lstm.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# Define early stopping with more patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch != 0 and epoch % 20 == 0: \n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Display the model summary\n",
    "lstm.summary()\n",
    "\n",
    "# Train the model\n",
    "history = lstm.fit(X_train, Y_train,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    epochs=50, batch_size=32, verbose=True, callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 55ms/step - loss: 7.0621e-07\n",
      "Evaluate:  6.257074574023136e-07\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step\n",
      "MSE desnormalized 0.00634218509225754\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 9.3053e-07\n",
      "Test Evaluate:  1.3233300251158653e-06\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step\n",
      "MSE desnormalized 0.013405891023802389\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate: \", lstm.evaluate(X_test, Y_test))\n",
    "pred = lstm.predict(X_test)\n",
    "pred_denormalized = scaler_Y.inverse_transform(pred.flatten(order='F').reshape(-1,1))\n",
    "Y_test_denormalized = scaler_Y.inverse_transform(Y_test.flatten(order='F').reshape(-1,1))\n",
    "mse = mean_squared_error(Y_test_denormalized, pred_denormalized)\n",
    "print(\"MSE desnormalized\", mse)\n",
    "\n",
    "print(\"Test Evaluate: \", lstm.evaluate(TestX_seq, TestY_seq))\n",
    "Testpred = lstm.predict(TestX_seq).flatten(order='F').reshape(-1,1)\n",
    "Testpred_denormalized = scaler_Y.inverse_transform(Testpred.flatten(order='F').reshape(-1,1))\n",
    "TestY_denormalized = scaler_Y.inverse_transform(TestY_seq.flatten(order='F').reshape(-1,1))\n",
    "mse = mean_squared_error(TestY_denormalized, Testpred_denormalized)\n",
    "print(\"MSE desnormalized\", mse)"
   ]
  },


  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## +50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">70,144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m70,144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,441</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m333,441\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">333,441</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m333,441\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 134ms/step - loss: 8.9763e-07 - val_loss: 4.3339e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 123ms/step - loss: 7.8681e-07 - val_loss: 3.8616e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 134ms/step - loss: 8.5287e-07 - val_loss: 3.9967e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 131ms/step - loss: 7.7131e-07 - val_loss: 4.0374e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 122ms/step - loss: 1.0510e-06 - val_loss: 3.7068e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 123ms/step - loss: 6.7749e-07 - val_loss: 3.3376e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 122ms/step - loss: 6.3765e-07 - val_loss: 3.5370e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 130ms/step - loss: 8.5103e-07 - val_loss: 3.6769e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 131ms/step - loss: 6.9613e-07 - val_loss: 4.2331e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 121ms/step - loss: 7.8571e-07 - val_loss: 4.5209e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 124ms/step - loss: 7.9801e-07 - val_loss: 4.9933e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 125ms/step - loss: 8.3618e-07 - val_loss: 3.0098e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 122ms/step - loss: 6.2519e-07 - val_loss: 4.1570e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 119ms/step - loss: 5.9522e-07 - val_loss: 3.2670e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 115ms/step - loss: 6.4801e-07 - val_loss: 3.8576e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 128ms/step - loss: 6.5128e-07 - val_loss: 3.1416e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 128ms/step - loss: 6.4292e-07 - val_loss: 2.5050e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 131ms/step - loss: 6.5243e-07 - val_loss: 3.2006e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 130ms/step - loss: 7.4177e-07 - val_loss: 3.7315e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 124ms/step - loss: 6.1622e-07 - val_loss: 2.4518e-06 - learning_rate: 2.5000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 138ms/step - loss: 4.0783e-07 - val_loss: 2.8068e-06 - learning_rate: 1.2500e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 114ms/step - loss: 4.3872e-07 - val_loss: 2.7794e-06 - learning_rate: 1.2500e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 111ms/step - loss: 4.5395e-07 - val_loss: 2.8299e-06 - learning_rate: 1.2500e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 118ms/step - loss: 4.9310e-07 - val_loss: 2.5452e-06 - learning_rate: 1.2500e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 122ms/step - loss: 4.4098e-07 - val_loss: 2.7098e-06 - learning_rate: 1.2500e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 118ms/step - loss: 4.2779e-07 - val_loss: 3.6148e-06 - learning_rate: 1.2500e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 120ms/step - loss: 4.5735e-07 - val_loss: 2.8384e-06 - learning_rate: 1.2500e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm.compile(loss='mse', optimizer=Adam(learning_rate=2.5000e-04))\n",
    "\n",
    "# Define early stopping with more patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch != 0 and epoch % 20 == 0: \n",
    "        return lr * 0.5\n",
    "    else:\n",
    "        return lr\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Display the model summary\n",
    "lstm.summary()\n",
    "\n",
    "# Train the model\n",
    "history = lstm.fit(X_train, Y_train,\n",
    "                    validation_data=(X_val, Y_val),\n",
    "                    epochs=50, batch_size=32, verbose=True, callbacks=[early_stopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 4.9371e-07\n",
      "Evaluate:  4.211565283185337e-07\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 58ms/step\n",
      "MSE desnormalized 0.004278467948630388\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - loss: 6.1460e-07\n",
      "Test Evaluate:  8.34140280403517e-07\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step\n",
      "MSE desnormalized 0.008450194524307841\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluate: \", lstm.evaluate(X_test, Y_test))\n",
    "pred = lstm.predict(X_test)\n",
    "pred_denormalized = scaler_Y.inverse_transform(pred.flatten(order='F').reshape(-1,1))\n",
    "Y_test_denormalized = scaler_Y.inverse_transform(Y_test.flatten(order='F').reshape(-1,1))\n",
    "mse = mean_squared_error(Y_test_denormalized, pred_denormalized)\n",
    "print(\"MSE desnormalized\", mse)\n",
    "\n",
    "print(\"Test Evaluate: \", lstm.evaluate(TestX_seq, TestY_seq))\n",
    "Testpred = lstm.predict(TestX_seq).flatten(order='F').reshape(-1,1)\n",
    "Testpred_denormalized = scaler_Y.inverse_transform(Testpred.flatten(order='F').reshape(-1,1))\n",
    "TestY_denormalized = scaler_Y.inverse_transform(TestY_seq.flatten(order='F').reshape(-1,1))\n",
    "mse = mean_squared_error(TestY_denormalized, Testpred_denormalized)\n",
    "print(\"MSE desnormalized\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.save('BestLSTM.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = load_model('BestLSTM.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprossesing -> X_train, X_test, X_val, Y_train, Y_test, Y_val\n",
    "# Train - 80% dos dias, Test - 10% dos dias, Validation - 10% dos dias\n",
    "train_size_per_day = int(0.8 * num_samples_per_day)\n",
    "val_size_per_day = int(0.1 * num_samples_per_day)\n",
    "test_size_per_day = num_samples_per_day - train_size_per_day - val_size_per_day\n",
    "\n",
    "X_train = X_seq[:train_size_per_day]\n",
    "Y_train = Y_seq[:train_size_per_day]\n",
    "\n",
    "X_val = X_seq[train_size_per_day : train_size_per_day+val_size_per_day]\n",
    "Y_val = Y_seq[train_size_per_day : train_size_per_day+val_size_per_day]\n",
    "\n",
    "X_test = X_seq[train_size_per_day+val_size_per_day:]\n",
    "Y_test = Y_seq[train_size_per_day+val_size_per_day:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m825/825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 18ms/step\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predict = lstm.predict(X_train).flatten(order='F').reshape((-1,1))\n",
    "val_predict = lstm.predict(X_val).flatten(order='F').reshape((-1,1))\n",
    "test_predict = lstm.predict(X_test).flatten(order='F').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4.653137991628704e-07\n",
      "Mean Absolute Error (MAE): 0.000484927327098023\n",
      "RMSE:  0.000682139134753952\n",
      "R2:  0.9999840505462633\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(train_predict, Y_train.flatten(order='F').reshape((-1,1)))\n",
    "mae = mean_absolute_error(train_predict, Y_train.flatten(order='F').reshape((-1,1)))\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_train.flatten(order='F').reshape((-1,1)), train_predict)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 2.843836275100394e-06\n",
      "Mean Absolute Error (MAE): 0.0007160842624035654\n",
      "RMSE:  0.001686367775753674\n",
      "R2:  0.9998956943608741\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(val_predict, Y_val.flatten(order='F').reshape((-1,1)))\n",
    "mae = mean_absolute_error(val_predict, Y_val.flatten(order='F').reshape((-1,1)))\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_val.flatten(order='F').reshape((-1,1)), val_predict.flatten(order='F').reshape((-1,1)))\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 4.2233860782129907e-07\n",
      "Mean Absolute Error (MAE): 0.00047450129855816116\n",
      "RMSE:  0.0006498758403120546\n",
      "R2:  0.9999854787696539\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(test_predict, Y_test.flatten(order='F').reshape((-1,1)))\n",
    "mae = mean_absolute_error(test_predict, Y_test.flatten(order='F').reshape((-1,1)))\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_test.flatten(order='F').reshape((-1,1)), test_predict.flatten(order='F').reshape((-1,1)))\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "Average execution time over 10 runs: 0.485283 seconds\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "\n",
    "# List to store elapsed times\n",
    "elapsed_times = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Code to measure\n",
    "    testpredict = lstm.predict(TestX_seq).flatten(order='F').reshape((-1, 1))\n",
    "\n",
    "    # End time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Append elapsed time to list\n",
    "    elapsed_times.append(elapsed_time)\n",
    "\n",
    "# Calculate average elapsed time\n",
    "average_elapsed_time = np.mean(elapsed_times)\n",
    "\n",
    "print(f\"Average execution time over {num_iterations} runs: {average_elapsed_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "testpredict = lstm.predict(TestX_seq).flatten(order='F').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 8.341399019076934e-07\n",
      "Mean Absolute Error (MAE): 0.0005650430684754193\n",
      "RMSE:  0.0009133125981325853\n",
      "R2:  0.999963235151765\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(testpredict, TestY_seq.flatten(order='F').reshape((-1,1)))\n",
    "mae = mean_absolute_error(testpredict, TestY_seq.flatten(order='F').reshape((-1,1)))\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(TestY_seq.flatten(order='F').reshape((-1,1)), testpredict)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"R2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE denormalized: 0.008450189777806018\n",
      "MAE denormalized: 0.05687158217300017\n",
      "RMSE denormalized: 0.0919249138036366\n",
      "R2 denormalized (same as normalized): 0.999963235151765\n"
     ]
    }
   ],
   "source": [
    "testpredict_denormalized = scaler_Y.inverse_transform(testpredict.flatten(order='F').reshape(-1, 1))\n",
    "TestY_denormalized = scaler_Y.inverse_transform(TestY_seq.flatten(order='F').reshape(-1, 1))\n",
    "\n",
    "mse_denormalized = mean_squared_error(TestY_denormalized, testpredict_denormalized)\n",
    "mae_denormalized = mean_absolute_error(TestY_denormalized, testpredict_denormalized)\n",
    "rmse_denormalized = np.sqrt(mse_denormalized)\n",
    "\n",
    "print(\"MSE denormalized:\", mse_denormalized)\n",
    "print(\"MAE denormalized:\", mae_denormalized)\n",
    "print(\"RMSE denormalized:\", rmse_denormalized)\n",
    "print(\"R2 denormalized (same as normalized):\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
